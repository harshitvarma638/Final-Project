{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d57a91-0a14-4020-84fa-6f6d52de3137",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow nltk seaborn nlpaug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e59def-37ca-43cd-aac1-5e1494fcf6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, GlobalMaxPool1D, Bidirectional, SpatialDropout1D\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, LearningRateScheduler\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.layers import Layer, Conv1D, MaxPooling1D, LayerNormalization\n",
    "from tensorflow.keras.optimizers import AdamW\n",
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd39b58-9b85-4aac-a746-f8c0231d653a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "uploaded = files.upload()\n",
    "\n",
    "df = pd.read_csv(\"all-data.csv\", header=None, encoding=\"ISO-8859-1\")\n",
    "df.columns = [\"label\", \"text\"]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c05cc1b-efc3-408c-9252-d8369a63afeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    # Keep meaningful punctuation like ! and ?\n",
    "    text = re.sub(r'[#$%&\\()*+,-./:<=>@\\\\^_`{|}~\\[\\]]', ' ', text)\n",
    "    # Handle multiple exclamation/question marks\n",
    "    text = re.sub(r'!+', ' ! ', text)\n",
    "    text = re.sub(r'\\?+', ' ? ', text)\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Tokenize, lemmatize and remove stopwords\n",
    "    words = text.split()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f4aece-ead5-4c1f-9343-0bc70fa6202e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"clean_text\"] = df[\"text\"].astype(str).apply(preprocess_text)\n",
    "\n",
    "# Check data distribution\n",
    "print(df[\"label\"].value_counts())\n",
    "\n",
    "# Encode labels\n",
    "label_mapping = {\"negative\": 0, \"neutral\": 1, \"positive\": 2}\n",
    "df[\"label\"] = df[\"label\"].astype(str).str.strip().map(label_mapping).fillna(1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8dd8c4-6d27-4b5a-99a4-8ae2eca36a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 10000  # Increased vocabulary size\n",
    "maxlen = 100  # Increased sequence length\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_features, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(df[\"clean_text\"])\n",
    "sequences = tokenizer.texts_to_sequences(df[\"clean_text\"])\n",
    "X = pad_sequences(sequences, maxlen=maxlen, padding='post', truncating='post')\n",
    "y = df[\"label\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a38f4d4-695a-4441-9b31-4338ce8741c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de479a86-1bbb-4560-b83e-e24977f3d5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://nlp.stanford.edu/data/glove.6B.zip\n",
    "!unzip glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4065aee-f840-4adc-b1bf-ddb45301012f",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 300  # Increased embedding dimension\n",
    "embedding_index = {}\n",
    "\n",
    "with open(\"glove.6B.300d.txt\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype=\"float32\")\n",
    "        embedding_index[word] = coefs\n",
    "\n",
    "embedding_matrix = np.zeros((max_features, embedding_dim))\n",
    "\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i < max_features:\n",
    "        embedding_vector = embedding_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a240cf-fe1b-4ac9-a151-890c418f9f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name=\"att_weight\", shape=(input_shape[-1], 1), initializer=\"normal\")\n",
    "        self.b = self.add_weight(name=\"att_bias\", shape=(1,), initializer=\"zeros\")\n",
    "        super(Attention, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        e = K.tanh(K.dot(x, self.W) + self.b)\n",
    "        a = K.softmax(e, axis=1)\n",
    "        output = x * a\n",
    "        return K.sum(output, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a9a5e2-e2e0-4ac2-bf07-c4c951289914",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlpaug.augmenter.word import SynonymAug\n",
    "\n",
    "def augment_data(texts, labels, augment_percentage=0.3):\n",
    "    aug = SynonymAug(aug_src='wordnet')\n",
    "    augmented_texts = []\n",
    "    augmented_labels = []\n",
    "\n",
    "    for i, (text, label) in enumerate(zip(texts, labels)):\n",
    "        if np.random.random() < augment_percentage:\n",
    "            try:\n",
    "                aug_text = aug.augment(text)[0]\n",
    "                augmented_texts.append(aug_text)\n",
    "                augmented_labels.append(label)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    return augmented_texts, augmented_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a168692d-4d38-4725-80b1-579d10300fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = [tokenizer.sequences_to_texts([seq])[0] for seq in X_train]\n",
    "aug_texts, aug_labels = augment_data(train_texts, y_train)\n",
    "\n",
    "# Tokenize and pad augmented data\n",
    "aug_sequences = tokenizer.texts_to_sequences(aug_texts)\n",
    "X_aug = pad_sequences(aug_sequences, maxlen=maxlen, padding='post', truncating='post')\n",
    "\n",
    "# Combine with original training data\n",
    "X_train_combined = np.vstack([X_train, X_aug])\n",
    "y_train_combined = np.concatenate([y_train, aug_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed962b46-ab81-4a24-bb63-b8fd561441f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train_combined), y=y_train_combined)\n",
    "class_weight_dict = dict(enumerate(class_weights))\n",
    "print(\"Class weights:\", class_weight_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d94f806-555e-40a6-be5f-afa3b3e37991",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_schedule(epoch):\n",
    "    lr = 3e-4\n",
    "    if epoch > 15:\n",
    "        lr *= 0.1\n",
    "    elif epoch > 8:\n",
    "        lr *= 0.5\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d6998c-8325-410a-9430-b731cc64c938",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Embedding(input_dim=max_features, output_dim=embedding_dim, weights=[embedding_matrix], trainable=False),\n",
    "    SpatialDropout1D(0.2),\n",
    "    Conv1D(128, kernel_size=3, padding='same', activation=\"relu\"),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Bidirectional(LSTM(128, return_sequences=True)),\n",
    "    Attention(),\n",
    "    LayerNormalization(),\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "    Dropout(0.2),\n",
    "    Dense(3, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea953b1e-cdf8-487c-b91c-8d2d8f709bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=AdamW(learning_rate=3e-4, weight_decay=1e-5),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a4bc3b-15b1-41e1-bcb8-72c58a8d248e",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=3, verbose=1)\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e8539e-8e3c-4f4e-a7f3-75c5373de53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    X_train_combined,\n",
    "    y_train_combined,\n",
    "    epochs=30,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=[early_stopping, reduce_lr, lr_scheduler],\n",
    "    class_weight=class_weight_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830fe3d5-71a7-4838-9378-61cf52e8ff25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test):\n",
    "    y_pred_proba = model.predict(X_test)\n",
    "    y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=[\"negative\", \"neutral\", \"positive\"]))\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"negative\", \"neutral\", \"positive\"], yticklabels=[\"negative\", \"neutral\", \"positive\"])\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a34408-2311-4458-a987-e23ed8c7e93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = evaluate_model(model, X_test, y_test)\n",
    "plot_confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Cell 22: Plot training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
